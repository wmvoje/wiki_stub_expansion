{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to identify possible ways of collecting this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Available repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "session = mwapi.Session('https://en.wikipedia.org', user_agent='williamvoje@gmail.com')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_request_data = session.get(action='query', titles ='git', prop = 'revisions', rvprop='content')\n",
    "page_request_data['query']['pages']['1771747']['revisions'][0]['contentmodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_page_data = page_request_data['query']['pages']['1771747']['revisions'][0]['*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = mwparserfromhell.parse(res[\"query\"][\"pages\"]['1771747']['revisions'][0]['*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.filter_wikilinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.filter_headings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.get_sections()[0].filter_wikilinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parsed.strip_code())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like mwparserfromhell will get the job done for parsing the data of raw information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_request_data = session.get(action='query', titles ='git', prop = 'revisions', rvprop='content')\n",
    "page_request_data['query']['pages']['1771747']['revisions'][0]['contentmodel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_request_data = session.get(action='query', titles ='Git', prop = 'revisions', \n",
    "                                rvprop='ids|flags|timestamp|comment|user|userid|content',\n",
    "                                rvlimit='max', rvstop='2015-07-01T11:03:28Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(page_request_data['query']['pages'].values()))['revisions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "session.get(action='query', titles ='git', prop = 'revisions', \n",
    "                    rvprop='ids|flags|timestamp|comment|user|userid|content',\n",
    "                    rvlimit='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have all of the tools we need to start collecting this data I'm going to test this on a small page\n",
    "\n",
    "\n",
    "First let's build out the tools we need to extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page = session.get(action='query', titles ='The_Faint', prop = 'revisions', \n",
    "                                rvprop='ids|flags|timestamp|comment|user|userid|content',\n",
    "                                rvlimit='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_page['query']['pages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes that you are only looking at single page\n",
    "\n",
    "page_title = 'The_Faint'\n",
    "path_to_data = '../data/20180618_Test_history_data/'\n",
    "\n",
    "headings = ['revid', 'parentid', 'user', 'userid', 'timestamp', 'comment', \n",
    "            'character_count', 'external_link_count', 'heading_count', 'wikilink_count', 'wikilinks']\n",
    "\n",
    "for rev_count, revision_set in enumerate(session.get(continuation=True, action='query', titles =page_title, prop = 'revisions', \n",
    "                                rvprop='ids|flags|timestamp|comment|user|userid|content',\n",
    "                                rvlimit='max')):\n",
    "    \n",
    "    # Make dataframe for storing information\n",
    "    \n",
    "    data_frame = pd.DataFrame(columns=headings)\n",
    "    \n",
    "\n",
    "    for count, revision in enumerate(next(iter(revision_set['query']['pages'].values()))['revisions']):\n",
    "\n",
    "        # Extract edit information \n",
    "        revid, parentid, user, userid, timestamp, comment = revision_information(revision)\n",
    "\n",
    "        # Extract information on the website itself\n",
    "        parsed = mwparserfromhell.parse(revision['*'])\n",
    "\n",
    "        character_count, external_link_count, heading_count, wikilink_count, wikilinks = parsed_article_metrics(parsed)\n",
    "        \n",
    "        data_frame.loc[count] = [ revid, parentid, user, userid, timestamp, comment,\n",
    "                                 character_count, external_link_count, heading_count, wikilink_count, wikilinks]\n",
    "        \n",
    "    # Dump the dataframe\n",
    "    data_frame.to_csv(os.path.join(path_to_data, page_title + \"_\" + str(rev_count) + '.csv'))\n",
    "        \n",
    "#         print(character_count, external_link_count, heading_count, wikilink_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame['wikilinks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revision_information(revision_json):\n",
    "    \"\"\" This will export \n",
    "    revid, parentid, user, userid, timestamp, comment\n",
    "    \"\"\"\n",
    "    return [revision['revid'], revision['parentid'], revision['user'], revision['userid'],\n",
    "           revision['timestamp'], revision['comment']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_article_metrics(parsed_article):\n",
    "    \"\"\"This should take a parsed article\n",
    "    Presently I'm using a mwparserfromhell\"\"\"\n",
    "    character_count = len(parsed_article.strip_code())\n",
    "    external_link_count = len(parsed_article.filter_external_links())\n",
    "    heading_count = len(parsed_article.filter_headings())\n",
    "    wikilink_count = len(set([str(link) for link in parsed_article.filter_wikilinks()]))            \n",
    "    \n",
    "    return character_count, external_link_count, heading_count, wikilink_count, parsed.filter_wikilinks()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revid, parentid, user, userid, annon, timestamp, comment = revision_information(revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set([str(link) for link in parsed.filter_wikilinks()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.filter_wikilinks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed.filter_wikilinks()[0].ipynb_checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process to complete\n",
    "\n",
    "# init the process with the webpage \n",
    "session = mwapi.Session('https://en.wikipedia.org', user_agent='williamvoje@gmail.com')\n",
    "\n",
    "# iterate through page revision history\n",
    "for page_revisions in \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidating all csvs in directory \n",
    "path_to_data = '../data/20180618_Test_history_data/'\n",
    "\n",
    "\n",
    "df = pd.concat(pd.read_csv(\n",
    "                \n",
    "                for f in os.listdir(path_to_data)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.path.join(path_to_data, f) for f in os.listdir(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[os.path.join(path_to_data, f) for f in os.listdir(path_to_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in os.walk(path_to_data):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = os.listdir(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in glob.iglob(path_to_data + '*.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(pd.read_csv(i) for i in glob.iglob(path_to_data + '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wikilinks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
