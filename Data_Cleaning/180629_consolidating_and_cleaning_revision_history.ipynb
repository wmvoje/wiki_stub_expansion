{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn \n",
    "import pathlib\n",
    "import dateutil\n",
    "import pytz\n",
    "import json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data descriptions and storage\n",
    "\n",
    "All data that were collected come from en.wikipedia. A description of the data and how it is stored is detailed below:\n",
    "\n",
    "**1. Revision history of pages with metadata from ~June 2015 - ~July 2018**\n",
    "\n",
    "* Description of data\n",
    "* Metadata includes: 'page_title', 'revid', 'parentid', 'user', 'userid', 'timestamp', 'comment', 'character_count',   'word_count', 'external_link_count', 'heading_count', 'wikifile_count', 'wikilink_count'\n",
    "* Data are stored in individual CSV files for each page - with each row corresponding registered revision\n",
    "* There is a unique CSV for \n",
    "    \n",
    "**2. Historical wiki link data for each page**\n",
    "    \n",
    "* Storage: single layer dictionary stored as a .json for each page\n",
    "* Storage Description: Keys are the title of a wikipage this linked to by the titular wikipage, value is a list of all revision IDs of the titular page which contain that wikilink\n",
    "    \n",
    "**3. Daily page views for each page from ~June 2015 - ~July 2018**\n",
    "\n",
    "* Data consists of 'page_title', 'timestamp', 'page_views'\n",
    "* Data is stored in a single CSV file\n",
    "    \n",
    "**4. Backlinks for each wikipedia page from a single day ~July 2018**\n",
    "\n",
    "* Description of data: This data enumarates all of the pages on wikpedia which point to a specific page\n",
    "* Storage: Single .json file \n",
    "* Storage description: .json dicitionary key is the wikipage and value is a list of tuples (page_ID, page_title) which each describe a single page which points to to this page\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Cleaning Data\n",
    "\n",
    "## Revision history\n",
    "### Importing and datetime conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_revisions = pathlib.Path('../data/test15/revisions/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some revisions which were collected were of old/dead/defunct pages. This means that they have very few versions. To excude these we only consider files which are larger than 200 bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_from_each_file = (pd.read_csv(f, encoding='UTF-16') for f in path_to_revisions.iterdir() if f.lstat().st_size>200)\n",
    "concatenated_df   = pd.concat(df_from_each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing out junk headers\n",
    "\n",
    "concatenated_df = concatenated_df[['character_count',\n",
    "                                   'comment', 'external_link_count', 'heading_count', 'page_title',\n",
    "                                   'parentid', 'revid', 'timestamp', 'user', 'userid', 'wikifile_count',\n",
    "                                   'wikilink_count', 'word_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidating each revision entry together\n",
    "\n",
    "concatenated_df = concatenated_df[-concatenated_df.timestamp.apply(lambda x: type(x) == float)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting timestamp data into the correct form\n",
    "\n",
    "concatenated_df = concatenated_df.reset_index(drop=True)\n",
    "concatenated_df.timestamp = concatenated_df.timestamp.apply(lambda x: dateutil.parser.DEFAULTPARSER.parse(x))\n",
    "concatenated_df.timestamp = pd.to_datetime(concatenated_df.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.sort_values(by=['page_title', 'timestamp'], inplace=True)\n",
    "concatenated_df = concatenated_df.set_index('timestamp')\n",
    "concatenated_df['index'] = [r for r in range(len(concatenated_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2792456"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing all but the latest edit a user made in a single day\n",
    "\n",
    "There are many cases where many revisions are being done by a single editor in a single day. These will skew pages as having much higher numbers of edits. Here we'll reduce those numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (df['index'][:-1] for (page, date, user), df in concatenated_df.groupby(['page_title', pd.Grouper(freq='D'), 'user'])\n",
    "                    if len(df) > 1)\n",
    "\n",
    "concatenated_df.reset_index(inplace=True)\n",
    "concatenated_df = concatenated_df.set_index('index').drop(pd.concat(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv('../data/test15/cleaned/revision_data.csv', chunksize= 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview = pd.read_csv('../data/test15/pageview_data.csv')\n",
    "\n",
    "# Converting the timestamp data that was provided by wikipeida into a pandas friendly format\n",
    "\n",
    "test = pageview['Unnamed: 0'].apply(lambda x: dateutil.parser.DEFAULTPARSER.parse(x))\n",
    "test = test.apply(lambda x: x.replace(tzinfo=pytz.utc))\n",
    "pageview['datetime'] = test\n",
    "pageview = pageview.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping pandas dataframe\n",
    "\n",
    "pageview = pd.melt(pageview, id_vars='datetime', value_vars=pageview.columns[:-1])\n",
    "pageview.columns = ['datetime', 'page_title', 'views']\n",
    "pageview.page_title = pageview.page_title.str.replace('_', ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages pre cleaning: 12619\n"
     ]
    }
   ],
   "source": [
    "print('Number of pages pre cleaning: {0}'.format(len(pageview.page_title.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll only consider pages which have more than 1/2 year worth of page view entries\n",
    "# If there is a large set of NaN entries the page is probably fake\n",
    "\n",
    "pageview = pageview.groupby('page_title').filter(lambda x: sum(x['views'].isna()) < len(x['views']) - 183)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages post cleaning: 9788\n"
     ]
    }
   ],
   "source": [
    "print('Number of pages post cleaning: {0}'.format(len(pageview.page_title.unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview.columns = ['timestamp', 'page_title', 'page_views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping cleaned data\n",
    "\n",
    "pageview.to_csv('../data/test15/cleaned/pageview.csv', chunksize= 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining pageview and revision history data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in revision data: 10771\n",
      "Number of pages pageview data: 9788\n",
      "Number of pages common to both: 8722\n"
     ]
    }
   ],
   "source": [
    "print('Number of pages in revision data: {0}'.format(len(concatenated_df.page_title.unique())))\n",
    "print('Number of pages pageview data: {0}'.format(len(pageview.page_title.unique())))\n",
    "\n",
    "list_of_pages = set(pageview.page_title) & set(concatenated_df.page_title)\n",
    "print('Number of pages common to both: {0}'.format(len(list_of_pages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure these datetimes are in the right position\n",
    "\n",
    "concatenated_df.timestamp = pd.to_datetime(concatenated_df.timestamp)\n",
    "pageview.timestamp = pd.to_datetime(pageview.timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting datasets to combine\n",
    "\n",
    "concatenated_df = concatenated_df[concatenated_df.page_title.isin(list_of_pages)]\n",
    "pageview = pageview[pageview.page_title.isin(list_of_pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.set_index(['page_title', 'timestamp'], inplace=True)\n",
    "pageview.set_index(['page_title', 'timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1517283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9629088"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pageview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\willi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "consolidated_historical_data = pd.concat([pageview, concatenated_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_historical_data = consolidated_historical_data.sort_index(level=1).sort_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11146371"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(consolidated_historical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated_historical_data.to_csv('../data/test15/cleaned/combined.csv', chunksize=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlink data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_pagelinks = pathlib.Path('../data/test15/links_pointing_to_pages.json')\n",
    "with open(path_to_pagelinks, 'r') as json_to_read:\n",
    "    path_to_page_links = json.load(json_to_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many kinds of d\n",
    "\n",
    "count_dictionary = dict()\n",
    "for key, pointers in path_to_page_links.items():\n",
    "    count_dictionary[key] = {'talk':0,\n",
    "                             'wiki_links': 0, \n",
    "                             'wikipedia' : 0,\n",
    "                             'user' : 0}\n",
    "    for point in pointers:\n",
    "        if 'talk:' in point[1]:\n",
    "            count_dictionary[key]['talk'] += 1\n",
    "        elif 'wikipedia:' in point[1]:\n",
    "            count_dictionary[key]['wikipedia'] += 1\n",
    "        elif 'user:' in point[1]:\n",
    "            count_dictionary[key]['user'] += 1\n",
    "        else:\n",
    "            count_dictionary[key]['wiki_links'] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointers_df = pd.DataFrame.from_dict(count_dictionary, orient='index')\n",
    "pointers_df.reset_index(inplace=True)\n",
    "pointers_df.columns = ['page_title', 'talk_link_count', 'wiki_link_count', 'wikipeida_count', 'user_count']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointers_df.to_csv(pathlib.Path('../data/test15/cleaned/pointers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_data = pd.read_csv('../data/test15/cleaned/revision_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "revision_data = revision_data.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_revisions = dict((page, {'revid' :df.iloc[0]['revid'],\n",
    "                                 'category': []}) for page, df in revision_data.groupby('page_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This goes through every .json file of wikilinks iterates through each page that was \n",
    "# at one point pointed to. Determines if it is a Category: link (easy as each category link has that string in it).\n",
    "# And then only pulls out those which are pointed to by the current revision of the article\n",
    "\n",
    "path_to_json = pathlib.Path('../data/test15/wikilinks/')\n",
    "for path in path_to_json.iterdir():\n",
    "    page_name = path.name.split('.json')[0]\n",
    "    try:\n",
    "        with open(path, 'r') as to_open:\n",
    "            temp_file = json.load(to_open)\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        revision = current_revisions[page_name]['revid']\n",
    "    except:\n",
    "        continue\n",
    "    keys = (key for key in temp_file.keys() if 'Category:' in key)\n",
    "    for key in keys:\n",
    "        if int(revision) in temp_file[key]:\n",
    "            current_revisions[page_name]['category'].append(key.split('gory:')[1])\n",
    "#             print(key)\n",
    "#     links = temp_file[revision]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dictionary into a dataframe\n",
    "\n",
    "categories = pd.DataFrame.from_dict(current_revisions, orient='index')\n",
    "categories.reset_index(inplace=True)\n",
    "categories.columns = ['page_title', 'revid', 'categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dumping the dataframe\n",
    "\n",
    "categories.to_csv('../data/test15/cleaned/category_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
